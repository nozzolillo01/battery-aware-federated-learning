# ğŸ”‹ Battery-Aware Federated Learning

Federated Learning system with battery-aware client selection, designed for large-scale simulations in mobile/IoT scenarios. It includes a realistic battery simulator, selection strategies, rich logging to Weights & Biases (W&B), and full reproducibility from `pyproject.toml`.

## ğŸŒŸ What it does (at a glance)

- Battery-based client selection (weight âˆ battery_level^Î±, default Î±=2)
- Configurable minimum battery threshold for eligibility
- Realistic per-client consumption/charging simulation
- Complete metrics: client-side training/validation and centralized server-side test
- Ready-made W&B charts: â€œAccuracy per roundâ€ and â€œLoss per roundâ€

## ğŸ“¦ Project structure

```
app_battery/
â”œâ”€â”€ my_awesome_app/
â”‚   â”œâ”€â”€ task.py              # CNN + CIFARâ€‘10 data loading + federated partitioning
â”‚   â”œâ”€â”€ battery_simulator.py # Battery simulation and fleet management
â”‚   â”œâ”€â”€ base_strategy.py     # Base strategy (random-like selection with energy constraints)
â”‚   â”œâ”€â”€ battery_strategy.py  # Batteryâ€‘aware strategy (weights âˆ b^Î±)
â”‚   â”œâ”€â”€ client_app.py        # Flower client (local train + validation)
â”‚   â”œâ”€â”€ server_app.py        # Flower server (centralized test)
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ pyproject.toml           # Flower/strategy/environment configuration
â”œâ”€â”€ run_sweep.sh             # (opt.) sweep script
â”œâ”€â”€ sweep_runs.csv           # (opt.) sweep history
â””â”€â”€ README.md
```

## ğŸ§  Data and splits: what we actually measure

- Dataset: CIFARâ€‘10 from Hugging Face (`uoft-cs/cifar10`).
- Federated partitioning: Dirichlet with `alpha=0.1` (nonâ€‘IID) on the training data via `flwr_datasets.FederatedDataset`.
- Client-side validation: for each client, its shard is split `train/test` 80/20 (seed=42). Our â€œvalâ€ is therefore 20% of the clientâ€™s local training shard.
- Server-side test: centralized CIFARâ€‘10 â€œtestâ€ split (10,000 images) evaluated in `server_app.get_evaluate_fn`.

Key metrics (names as in W&B logs):
- train_accuracy_client, train_loss_client: from `aggregate_fit` (sample-weighted average across client results)
- val_accuracy_client, val_loss_client: from clientsâ€™ `aggregate_evaluate` (client key `accuracy` â†’ renamed to `val_accuracy_client`)
- test_accuracy_server, test_loss_server: from centralized server evaluation
- Others: battery_avg, battery_min, fairness_jain, total_energy, selected_clients, deaths_clients, etc.

Why are val_accuracy_client and test_accuracy_server often similar?
- They use the same global model and the same evaluation transforms (Normalize/ToTensor).
- The aggregated client validation (weighted average over many clients) tends to approximate an â€œaverageâ€ distribution that is close to the centralized test. Seeing similar trends is expected.

Why is train accuracy high from the very beginning?
- Shards are nonâ€‘IID (Dirichlet Î±=0.1) â†’ class imbalance and easier local distributions.
- Multiple local epochs on the same shard cause local overfitting (train accuracy can exceed 0.9) while val/test remain around 0.2â€“0.4.

How to differentiate curves / reduce overfitting
- Reduce `local-epochs` (1â€“2) or increase clients per round.
- Add data augmentation for train only; keep eval with normalization only.
- Add regularization (e.g., SGD `weight_decay`) or dropout.

## â–¶ï¸ How to run

Requirements: Python 3.8+, pip. Optional: W&B account.

1) Install (recommended in a virtualenv)
```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .
```

2) Start the simulation (uses `pyproject.toml` defaults)
```bash
flwr run .
```

Useful options (from `pyproject.toml`)
```toml
[tool.flwr.app.config]
num-server-rounds = 10
fraction-fit = 1.0
local-epochs = 3
min-battery-threshold = 0.2
alpha = 2
strategy = 1  # 0=base, 1=battery_aware

[tool.flwr.federations]
default = "small-simulation"

[tool.flwr.federations.small-simulation]
options.num-supernodes = 50
```

You can pick a different federation with `--federation` (e.g., `medium-simulation`). The number of simulated clients is read from `options.num-supernodes`. Alternatively, pass `--num-supernodes N` on the CLI (the server parses it in `server_app.get_num_supernodes_from_config`).

Disable W&B: export `WANDB_DISABLED=true` or run `wandb login` to have online charts. In code, `WANDB_SILENT` is set to avoid noisy console logs.

## ğŸ“Š Monitoring on W&B

Youâ€™ll find:
- chart/accuracy_per_round: train_accuracy_client, val_accuracy_client, test_accuracy_server
- chart/loss_per_round: train_loss_client, val_loss_client, test_loss_server
- Per-round tables with selection probabilities, battery levels, selected clients, deaths, etc.

Run names are `BASE-run-YYYY-mm-dd_HH:MM:SS` or `BATTERY-run-...` depending on the strategy.

## ğŸ”§ Quick customizations

- Strategy: change `strategy` (0 base, 1 batteryâ€‘aware); for the batteryâ€‘aware strategy, tune `alpha` (higher â‡’ more bias toward high-battery clients).
- Battery threshold: `min-battery-threshold`.
- Transforms: see `task.get_transforms()` and add augmentation for train only.

## ğŸ“„ License

Apacheâ€‘2.0. See `LICENSE`.

## ğŸ”¨ Stack

- [Flower](https://flower.ai/) â€¢ [PyTorch](https://pytorch.org/) â€¢ [flwr-datasets](https://github.com/adap/flower/tree/main/baselines/flwr_datasets) â€¢ [Weights & Biases](https://wandb.ai/) â€¢ CIFARâ€‘10

â€”

Optimized for experimenting with energyâ€‘aware federated learning strategies with clear metrics and reproducibility.